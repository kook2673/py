# -*- coding: utf-8 -*-
"""
1ë¶„/3ë¶„/5ë¶„ ìŠ¤ìº˜í•‘(ì¼ë°˜í™” MA) ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
- ë¹ ë¥¸/ëŠë¦° MA ê¸°ê°„, ì†ì ˆ ë¹„ìœ¨, ê±°ë˜ëŸ‰ ë°°ìˆ˜, í•„í„° ìµœì†Œ í†µê³¼ ê°œìˆ˜, íƒ€ì„í”„ë ˆì„(1/3/5ë¶„)ì„ ê·¸ë¦¬ë“œ íƒìƒ‰
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (ë©€í‹°í”„ë¡œì„¸ìŠ¤)
- ì§„í–‰ìƒíƒœë¥¼ JSONì— ì €ì¥í•˜ì—¬, ì¤‘ë‹¨ í›„ ì¬ì‹¤í–‰ ì‹œ ì´ì–´ì„œ ì§„í–‰
- ê°œë³„ ì „ëµ ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ê³ , ìµœì¢… ì¢…í•© ê²°ê³¼ CSV/JSONìœ¼ë¡œ ì €ì¥

ì‚¬ìš©ë²•:
  python Scalp_1m_5MA20MA_Optimizer.py
"""

import os
import json
import glob
from datetime import datetime
import itertools
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

import numpy as np
import pandas as pd

from Scalp_1m_5MA20MA_Backtest import backtest

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€)
script_dir = os.path.dirname(os.path.abspath(__file__))
PROGRESS_FILE = os.path.join(script_dir, 'logs', 'Scalp_Opt_Progress.json')
RESULTS_CSV = os.path.join(script_dir, 'logs', 'Scalp_Opt_Results.csv')
RESULTS_JSON = os.path.join(script_dir, 'logs', 'Scalp_Opt_Results.json')
REALTIME_LOG = os.path.join(script_dir, 'logs', 'Scalp_Opt_Realtime.log')

# ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ í•  ë°ì´í„°í”„ë ˆì„ (ê° ì›Œì»¤ì—ì„œ 1íšŒë§Œ ë¡œë“œ)
_SHARED_DF = None


def _init_worker(data_dir: str, csv_pattern: str):
    global _SHARED_DF
    try:
        files = glob.glob(os.path.join(data_dir, csv_pattern))
        dfs = []
        for f in sorted(files):
            df = pd.read_csv(f, index_col='timestamp', parse_dates=True)
            dfs.append(df)
        _SHARED_DF = pd.concat(dfs).sort_index().drop_duplicates()
    except Exception as e:
        _SHARED_DF = None


def _run_single(params):
    """ì›Œì»¤ì—ì„œ ì‹¤í–‰ë  ë‹¨ì¼ ì¡°í•© ì‹¤í–‰ í•¨ìˆ˜.
    params: (fast_w, slow_w, sl, vm, mfc, tfm)
    """
    global _SHARED_DF
    fast_w, slow_w, sl, vm, mfc, tfm = params

    key = f'f{fast_w}_s{slow_w}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'

    if _SHARED_DF is None:
        raise RuntimeError('Shared DF not initialized in worker')

    result = backtest(
        _SHARED_DF,
        stop_loss_pct=sl,
        fee=0.0005,
        leverage=10,
        volume_window=20,
        volume_multiplier=vm,
        min_pass_filters=mfc,
        risk_fraction=1.0,
        fast_ma_window=fast_w,
        slow_ma_window=slow_w,
        timeframe_minutes=tfm,
    )

    out = {
        'key': key,
        'fast_ma': fast_w,
        'slow_ma': slow_w,
        'stop_loss_pct': sl,
        'volume_multiplier': vm,
        'min_pass_filters': mfc,
        'timeframe_minutes': tfm,
        'total_return': result['total_return'],
        'mdd': result['mdd'],
        'final_capital': result['final_capital'],
        'trades': len([t for t in result['trades'] if t['type'] != 'LONG_ENTRY'])
    }
    return out


def load_progress():
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    return {}


def save_progress(progress: dict):
    os.makedirs(os.path.dirname(PROGRESS_FILE), exist_ok=True)
    # ë°±ì—… ì €ì¥
    try:
        if os.path.exists(PROGRESS_FILE):
            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
            os.replace(PROGRESS_FILE, PROGRESS_FILE + f'.bak')
    except Exception:
        pass
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def log_realtime(message: str):
    """ì‹¤ì‹œê°„ ì§„í–‰ìƒí™©ì„ íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œ ì¶œë ¥"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_message = f"[{timestamp}] {message}"
    
    # ì½˜ì†” ì¶œë ¥
    print(log_message)
    
    # íŒŒì¼ì— ì €ì¥
    try:
        os.makedirs(os.path.dirname(REALTIME_LOG), exist_ok=True)
        with open(REALTIME_LOG, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    except Exception as e:
        print(f"âš ï¸ ì‹¤ì‹œê°„ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def generate_param_grid():
    # ì¡°ì • ê°€ëŠ¥: ë²”ìœ„ë¥¼ ë„“íˆë©´ ì‹œê°„ì´ ê¸‰ì¦
    fast_windows = list(range(3, 21))        # 5~20
    slow_windows = list(range(20, 61, 5))    # 20~60 step=5
    # ì†ì ˆ ë¹„ìœ¨ì„ ë” ì´˜ì´˜í•˜ê²Œ: 0.03%~0.30% ê·¼ì²˜ êµ¬ê°„ í¬í•¨
    stop_losses = [0.0003, 0.0005, 0.0007, 0.0010, 0.0015, 0.0020, 0.0030]
    vol_multipliers = [1.0, 1.1, 1.2, 1.3]
    min_filter_counts = list(range(0, 11, 2))
    timeframes = [1, 3, 5]  # 1ë¶„, 3ë¶„, 5ë¶„ íƒìƒ‰

    # fast < slow ì¡°í•©ë§Œ ì‚¬ìš©
    combos = [(f, s) for f in fast_windows for s in slow_windows if f < s]

    # (f, s, sl, vm, mfc, tfm) íŠœí”Œ ëª©ë¡ ë°˜í™˜
    grid = [
        (f, s, sl, vm, mfc, tfm)
        for (f, s), sl, vm, mfc, tfm in itertools.product(
            combos, stop_losses, vol_multipliers, min_filter_counts, timeframes
        )
    ]
    return grid


def main():
    log_realtime('ğŸš€ ìŠ¤ìº˜í•‘ ìµœì í™” ì‹œì‘ (ë³‘ë ¬, ì¬ê°œ ì§€ì›)')
    log_realtime('=' * 60)

    # ë°ì´í„° ê²½ë¡œ (ì›Œì»¤ ì´ˆê¸°í™”ìš©)
    data_dir = os.path.join(script_dir, 'data', 'BTC_USDT', '1m')
    csv_pattern = '2025-03.csv'  # í•„ìš” ì‹œ ë³€ê²½

    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    grid = generate_param_grid()
    total = len(grid)
    log_realtime(f'ì´ ì¡°í•© ìˆ˜: {total:,}ê°œ')

    # ì§„í–‰ìƒíƒœ ë¶ˆëŸ¬ì˜¤ê¸°
    progress = load_progress()
    done_set = set(progress.get('done_keys', []))

    # ë¯¸ì™„ë£Œ ì¡°í•©ë§Œ í•„í„°ë§
    pending = []
    for f, s, sl, vm, mfc, tfm in grid:
        key = f'f{f}_s{s}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'
        if key not in done_set:
            pending.append((f, s, sl, vm, mfc, tfm))
    log_realtime(f'ëŒ€ê¸° ì‘ì—…: {len(pending):,}ê°œ (ì´ë¯¸ ì™„ë£Œ: {len(done_set):,}ê°œ)')

    if not pending:
        log_realtime('âœ… ì§„í–‰í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì™„ë£Œ)')
        return

    # ì›Œì»¤ ìˆ˜ ê²°ì • (5600X 6ì½”ì–´ 12ìŠ¤ë ˆë“œì— ìµœì í™”)
    max_workers = max(1, min((multiprocessing.cpu_count() or 2) - 1, 12))
    log_realtime(f'ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ')

    results = []

    # ì›Œì»¤ ì´ˆê¸°í™”(ë°ì´í„° 1íšŒ ë¡œë“œ)
    with ProcessPoolExecutor(max_workers=max_workers, initializer=_init_worker, initargs=(data_dir, csv_pattern)) as ex:
        futures = {ex.submit(_run_single, p): p for p in pending}
        for i, fut in enumerate(as_completed(futures), start=1):
            params = futures[fut]
            try:
                out = fut.result()
                results.append(out)
                done_set.add(out['key'])

                # ì§„í–‰ì¤‘ ì¶œë ¥
                sign = 'ğŸŸ¢' if out['total_return'] > 0 else 'ğŸ”´'
                log_realtime(f"{sign} {i}/{len(pending)} | tf={out['timeframe_minutes']}m, f={out['fast_ma']}, s={out['slow_ma']}, sl={out['stop_loss_pct']*100:.2f}%, vm={out['volume_multiplier']:.2f}, mfc={out['min_pass_filters']:02d} => ret={out['total_return']:+.2f}% MDD={out['mdd']:.2f}%")

            except Exception as e:
                # ì˜¤ë¥˜ë‚œ ì¡°í•©ë„ í‚¤ ê¸°ë¡í•´ ì¤‘ë³µ ì‹œë„ ë°©ì§€ (ì›ì¹˜ ì•Šìœ¼ë©´ ì œì™¸)
                f, s, sl, vm, mfc, tfm = params
                key = f'f{f}_s{s}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'
                log_realtime(f'âŒ ì˜¤ë¥˜: {key} - {e}')
                done_set.add(key)

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥(ì¦ë¶„)
            progress['done_keys'] = list(done_set)
            progress['last_time'] = datetime.now().isoformat()
            progress['total'] = total
            save_progress(progress)

            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            try:
                os.makedirs(os.path.dirname(RESULTS_CSV), exist_ok=True)
                pd.DataFrame(results).to_csv(RESULTS_CSV, index=False, encoding='utf-8-sig')
                with open(RESULTS_JSON, 'w', encoding='utf-8') as f:
                    json.dump({'results': results}, f, ensure_ascii=False, indent=2)
            except Exception as e:
                log_realtime(f'âš ï¸ ì¤‘ê°„ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}')

    # ìµœì¢… ìš”ì•½
    if results:
        df_r = pd.DataFrame(results)
        best_by_return = df_r.loc[df_r['total_return'].idxmax()]
        best_by_mdd = df_r.loc[df_r['mdd'].idxmin()]
        log_realtime('\nâœ… ìµœì í™” ì™„ë£Œ')
        log_realtime('=' * 60)
        log_realtime('ìˆ˜ìµë¥  ìµœê³  ì „ëµ:')
        log_realtime(str(best_by_return))
        log_realtime('\nMDD ìµœì†Œ ì „ëµ:')
        log_realtime(str(best_by_mdd))
        # ìµœì¢… ì €ì¥(ë®ì–´ì“°ê¸°)
        df_r.to_csv(RESULTS_CSV, index=False, encoding='utf-8-sig')
        with open(RESULTS_JSON, 'w', encoding='utf-8') as f:
            json.dump({'results': results, 'summary': {
                'best_by_return': best_by_return.to_dict(),
                'best_by_mdd': best_by_mdd.to_dict(),
            }}, f, ensure_ascii=False, indent=2)
    else:
        log_realtime('âŒ ê²°ê³¼ ì—†ìŒ')


if __name__ == '__main__':
    main()
