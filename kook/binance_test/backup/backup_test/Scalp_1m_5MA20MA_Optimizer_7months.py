# -*- coding: utf-8 -*-
"""
1ë¶„/3ë¶„/5ë¶„ ìŠ¤ìº˜í•‘(ì¼ë°˜í™” MA) ìµœì í™” ìŠ¤í¬ë¦½íŠ¸ - 7ê°œì›” í†µí•© ë²„ì „
- 2025ë…„ 1ì›”ë¶€í„° 7ì›”ê¹Œì§€ ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì‚°í•˜ì—¬ ìµœì í™”
- ë¹ ë¥¸/ëŠë¦° MA ê¸°ê°„, ì†ì ˆ ë¹„ìœ¨, ê±°ë˜ëŸ‰ ë°°ìˆ˜, í•„í„° ìµœì†Œ í†µê³¼ ê°œìˆ˜, íƒ€ì„í”„ë ˆì„(1/3/5ë¶„)ì„ ê·¸ë¦¬ë“œ íƒìƒ‰
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (ë©€í‹°í”„ë¡œì„¸ìŠ¤)
- ì§„í–‰ìƒíƒœë¥¼ JSONì— ì €ì¥í•˜ì—¬, ì¤‘ë‹¨ í›„ ì¬ì‹¤í–‰ ì‹œ ì´ì–´ì„œ ì§„í–‰
- ê°œë³„ ì „ëµ ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ê³ , ìµœì¢… ì¢…í•© ê²°ê³¼ CSV/JSONìœ¼ë¡œ ì €ì¥

ì‚¬ìš©ë²•:
  python Scalp_1m_5MA20MA_Optimizer_7months.py
"""

import os
import json
import glob
from datetime import datetime
import itertools
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

import numpy as np
import pandas as pd

from Scalp_1m_5MA20MA_Backtest import backtest

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€)
script_dir = os.path.dirname(os.path.abspath(__file__))
PROGRESS_FILE = os.path.join(script_dir, 'logs', 'Scalp_Opt_7months_Progress.json')
RESULTS_CSV = os.path.join(script_dir, 'logs', 'Scalp_Opt_7months_Results.csv')
RESULTS_JSON = os.path.join(script_dir, 'logs', 'Scalp_Opt_7months_Results.json')
REALTIME_LOG = os.path.join(script_dir, 'logs', 'Scalp_Opt_7months_Realtime.log')
# í”ŒëŸ¬ìŠ¤ ê²°ê³¼ë§Œ ì €ì¥í•˜ëŠ” íŒŒì¼ë“¤
POSITIVE_RESULTS_CSV = os.path.join(script_dir, 'logs', 'Scalp_Opt_7months_Positive_Results.csv')
POSITIVE_RESULTS_JSON = os.path.join(script_dir, 'logs', 'Scalp_Opt_7months_Positive_Results.json')

# ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ í•  ë°ì´í„°í”„ë ˆì„ (ê° ì›Œì»¤ì—ì„œ 1íšŒë§Œ ë¡œë“œ)
_SHARED_DF = None


def _init_worker(data_dir: str):
    """ì›Œì»¤ ì´ˆê¸°í™”: 7ê°œì›” ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¡œë“œ"""
    global _SHARED_DF
    try:
        print(f"ğŸ“¥ ì›Œì»¤ì—ì„œ 7ê°œì›” ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # 2025ë…„ 1ì›”ë¶€í„° 7ì›”ê¹Œì§€ ëª¨ë“  CSV íŒŒì¼ ì°¾ê¸°
        all_files = []
        for month in range(1, 8):
            month_pattern = f'2025-{month:02d}.csv'
            files = glob.glob(os.path.join(data_dir, month_pattern))
            all_files.extend(files)
        
        if not all_files:
            raise RuntimeError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        
        print(f"ğŸ“ ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ëª¨ë“  íŒŒì¼ì„ ìˆœì„œëŒ€ë¡œ ë¡œë“œí•˜ê³  í•©ì¹˜ê¸°
        dfs = []
        for f in sorted(all_files):
            month_name = os.path.basename(f).split('.')[0]
            print(f"ğŸ“– {month_name} ë°ì´í„° ë¡œë“œ ì¤‘...")
            df = pd.read_csv(f, index_col='timestamp', parse_dates=True)
            dfs.append(df)
            print(f"âœ… {month_name}: {len(df):,}ê°œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        _SHARED_DF = pd.concat(dfs).sort_index().drop_duplicates()
        print(f"ğŸ‰ 7ê°œì›” í†µí•© ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {len(_SHARED_DF):,}ê°œ ë°ì´í„°")
        
    except Exception as e:
        _SHARED_DF = None
        raise RuntimeError(f'ì›Œì»¤ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}')


def _run_single(params):
    """ì›Œì»¤ì—ì„œ ì‹¤í–‰ë  ë‹¨ì¼ ì¡°í•© ì‹¤í–‰ í•¨ìˆ˜.
    params: (fast_w, slow_w, sl, vm, mfc, tfm)
    """
    global _SHARED_DF
    fast_w, slow_w, sl, vm, mfc, tfm = params

    key = f'f{fast_w}_s{slow_w}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'

    if _SHARED_DF is None:
        raise RuntimeError('Shared DF not initialized in worker')

    # 7ê°œì›” í†µí•© ë°ì´í„°ë¡œ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = backtest(
        _SHARED_DF,
        stop_loss_pct=sl,
        fee=0.0005,
        leverage=10,
        volume_window=20,
        volume_multiplier=vm,
        min_pass_filters=mfc,
        risk_fraction=1.0,
        fast_ma_window=fast_w,
        slow_ma_window=slow_w,
        timeframe_minutes=tfm,
    )

    out = {
        'key': key,
        'fast_ma': fast_w,
        'slow_ma': slow_w,
        'stop_loss_pct': sl,
        'volume_multiplier': vm,
        'min_pass_filters': mfc,
        'timeframe_minutes': tfm,
        'total_return': result['total_return'],
        'mdd': result['mdd'],
        'final_capital': result['final_capital'],
        'trades': len([t for t in result['trades'] if t['type'] != 'LONG_ENTRY']),
        'win_rate': calculate_win_rate(result['trades']),
        'avg_trade_return': calculate_avg_trade_return(result['trades']),
        'max_consecutive_losses': calculate_max_consecutive_losses(result['trades']),
        'sharpe_ratio': calculate_sharpe_ratio(result['trades']),
        'data_points': len(_SHARED_DF),
        'test_period': '2025-01 to 2025-07'
    }
    return out


def calculate_win_rate(trades):
    """ìŠ¹ë¥  ê³„ì‚°"""
    if not trades:
        return 0.0
    
    winning_trades = [t for t in trades if t.get('pnl', 0) > 0]
    return len(winning_trades) / len(trades) * 100


def calculate_avg_trade_return(trades):
    """í‰ê·  ê±°ë˜ ìˆ˜ìµë¥  ê³„ì‚°"""
    if not trades:
        return 0.0
    
    total_pnl = sum(t.get('pnl', 0) for t in trades)
    return total_pnl / len(trades)


def calculate_max_consecutive_losses(trades):
    """ìµœëŒ€ ì—°ì† ì†ì‹¤ ê³„ì‚°"""
    if not trades:
        return 0
    
    max_consecutive = 0
    current_consecutive = 0
    
    for trade in trades:
        if trade.get('pnl', 0) < 0:
            current_consecutive += 1
            max_consecutive = max(max_consecutive, current_consecutive)
        else:
            current_consecutive = 0
    
    return max_consecutive


def calculate_sharpe_ratio(trades):
    """ìƒ¤í”„ ë¹„ìœ¨ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
    if not trades:
        return 0.0
    
    returns = [t.get('pnl', 0) for t in trades]
    if not returns:
        return 0.0
    
    avg_return = sum(returns) / len(returns)
    variance = sum((r - avg_return) ** 2 for r in returns) / len(returns)
    
    if variance == 0:
        return 0.0
    
    return avg_return / (variance ** 0.5)


def load_progress():
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    return {}


def save_progress(progress: dict):
    os.makedirs(os.path.dirname(PROGRESS_FILE), exist_ok=True)
    # ë°±ì—… ì €ì¥
    try:
        if os.path.exists(PROGRESS_FILE):
            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
            os.replace(PROGRESS_FILE, PROGRESS_FILE + f'.bak_{ts}')
    except Exception:
        pass
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def log_realtime(message: str):
    """ì‹¤ì‹œê°„ ì§„í–‰ìƒí™©ì„ íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œ ì¶œë ¥"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_message = f"[{timestamp}] {message}"
    
    # ì½˜ì†” ì¶œë ¥
    print(log_message)
    
    # íŒŒì¼ì— ì €ì¥
    try:
        os.makedirs(os.path.dirname(REALTIME_LOG), exist_ok=True)
        with open(REALTIME_LOG, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    except Exception as e:
        print(f"âš ï¸ ì‹¤ì‹œê°„ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def generate_param_grid():
    # ì¡°ì • ê°€ëŠ¥: ë²”ìœ„ë¥¼ ë„“íˆë©´ ì‹œê°„ì´ ê¸‰ì¦
    fast_windows = list(range(3, 21))        # 3~20
    slow_windows = list(range(20, 61, 5))    # 20~60 step=5
    # ì†ì ˆ ë¹„ìœ¨ì„ ë” ì´˜ì´˜í•˜ê²Œ: 0.03%~0.30% ê·¼ì²˜ êµ¬ê°„ í¬í•¨
    stop_losses = [0.0003, 0.0005, 0.0007, 0.0010, 0.0015, 0.0020, 0.0030]
    vol_multipliers = [1.0, 1.1, 1.2, 1.3]
    min_filter_counts = list(range(0, 11, 2))
    timeframes = [1, 3, 5]  # 1ë¶„, 3ë¶„, 5ë¶„ íƒìƒ‰

    # fast < slow ì¡°í•©ë§Œ ì‚¬ìš©
    combos = [(f, s) for f in fast_windows for s in slow_windows if f < s]

    # (f, s, sl, vm, mfc, tfm) íŠœí”Œ ëª©ë¡ ë°˜í™˜
    grid = [
        (f, s, sl, vm, mfc, tfm)
        for (f, s), sl, vm, mfc, tfm in itertools.product(
            combos, stop_losses, vol_multipliers, min_filter_counts, timeframes
        )
    ]
    return grid


def main():
    log_realtime('ğŸš€ ìŠ¤ìº˜í•‘ ìµœì í™” ì‹œì‘ (7ê°œì›” í†µí•© ë°ì´í„°, ë³‘ë ¬, ì¬ê°œ ì§€ì›)')
    log_realtime('=' * 70)

    # ë°ì´í„° ê²½ë¡œ (ì›Œì»¤ ì´ˆê¸°í™”ìš©)
    data_dir = os.path.join(script_dir, 'data', 'BTC_USDT', '1m')
    
    # 7ê°œì›” ë°ì´í„° ì¡´ì¬ í™•ì¸
    available_months = []
    for month in range(1, 8):
        month_pattern = f'2025-{month:02d}.csv'
        files = glob.glob(os.path.join(data_dir, month_pattern))
        if files:
            available_months.append(f'2025-{month:02d}')
    
    if not available_months:
        log_realtime(f"âŒ 7ê°œì›” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    log_realtime(f"ğŸ“… ì‚¬ìš© ê°€ëŠ¥í•œ ì›”: {', '.join(available_months)}")
    log_realtime(f"ğŸ“Š ì´ {len(available_months)}ê°œ ì›” ë°ì´í„°ë¡œ ìµœì í™” ì§„í–‰")

    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    grid = generate_param_grid()
    total = len(grid)
    log_realtime(f'ì´ ì¡°í•© ìˆ˜: {total:,}ê°œ')

    # ì§„í–‰ìƒíƒœ ë¶ˆëŸ¬ì˜¤ê¸°
    progress = load_progress()
    done_set = set(progress.get('done_keys', []))

    # ë¯¸ì™„ë£Œ ì¡°í•©ë§Œ í•„í„°ë§
    pending = []
    for f, s, sl, vm, mfc, tfm in grid:
        key = f'f{f}_s{s}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'
        if key not in done_set:
            pending.append((f, s, sl, vm, mfc, tfm))
    log_realtime(f'ëŒ€ê¸° ì‘ì—…: {len(pending):,}ê°œ (ì´ë¯¸ ì™„ë£Œ: {len(done_set):,}ê°œ)')

    if not pending:
        log_realtime('âœ… ì§„í–‰í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì™„ë£Œ)')
        return

    # ì›Œì»¤ ìˆ˜ ê²°ì • (7800X3D 8ì½”ì–´ 16ìŠ¤ë ˆë“œì— ìµœì í™”)
    max_workers = max(1, min((multiprocessing.cpu_count() or 2) - 1, 13))
    log_realtime(f'ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ')

    results = []

    # ì›Œì»¤ ì´ˆê¸°í™”(7ê°œì›” ë°ì´í„° 1íšŒ ë¡œë“œ)
    with ProcessPoolExecutor(max_workers=max_workers, initializer=_init_worker, initargs=(data_dir,)) as ex:
        futures = {ex.submit(_run_single, p): p for p in pending}
        for i, fut in enumerate(as_completed(futures), start=1):
            params = futures[fut]
            try:
                out = fut.result()
                results.append(out)
                done_set.add(out['key'])

                # ì§„í–‰ì¤‘ ì¶œë ¥
                sign = 'ğŸŸ¢' if out['total_return'] > 0 else 'ğŸ”´'
                log_realtime(f"{sign} {i}/{len(pending)} | tf={out['timeframe_minutes']}m, f={out['fast_ma']}, s={out['slow_ma']}, sl={out['stop_loss_pct']*100:.2f}%, vm={out['volume_multiplier']:.2f}, mfc={out['min_pass_filters']:02d} => ret={out['total_return']:+.2f}% MDD={out['mdd']:.2f}%")

            except Exception as e:
                # ì˜¤ë¥˜ë‚œ ì¡°í•©ë„ í‚¤ ê¸°ë¡í•´ ì¤‘ë³µ ì‹œë„ ë°©ì§€
                f, s, sl, vm, mfc, tfm = params
                key = f'f{f}_s{s}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'
                log_realtime(f'âŒ ì˜¤ë¥˜: {key} - {e}')
                done_set.add(key)

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥(ì¦ë¶„)
            if i % 100 == 0 or i == len(pending):
                progress['done_keys'] = list(done_set)
                progress['last_time'] = datetime.now().isoformat()
                progress['total'] = total
                progress['completed'] = i
                save_progress(progress)

            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            if i % 1000 == 0 or i == len(pending):
                try:
                    os.makedirs(os.path.dirname(RESULTS_CSV), exist_ok=True)
                    
                    # ì „ì²´ ê²°ê³¼ ì €ì¥
                    pd.DataFrame(results).to_csv(RESULTS_CSV, index=False, encoding='utf-8-sig')
                    with open(RESULTS_JSON, 'w', encoding='utf-8') as f:
                        json.dump({'results': results}, f, ensure_ascii=False, indent=2)
                    
                    # í”ŒëŸ¬ìŠ¤ ê²°ê³¼ë§Œ ë”°ë¡œ ì €ì¥
                    positive_results = [r for r in results if r['total_return'] > 0]
                    if positive_results:
                        pd.DataFrame(positive_results).to_csv(POSITIVE_RESULTS_CSV, index=False, encoding='utf-8-sig')
                        with open(POSITIVE_RESULTS_JSON, 'w', encoding='utf-8') as f:
                            json.dump({
                                'positive_results': positive_results,
                                'count': len(positive_results),
                                'last_updated': datetime.now().isoformat()
                            }, f, ensure_ascii=False, indent=2)
                        log_realtime(f'ğŸ’¾ ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {i:,}ê°œ (í”ŒëŸ¬ìŠ¤: {len(positive_results):,}ê°œ)')
                    else:
                        log_realtime(f'ğŸ’¾ ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {i:,}ê°œ (í”ŒëŸ¬ìŠ¤: 0ê°œ)')
                        
                except Exception as e:
                    log_realtime(f'âš ï¸ ì¤‘ê°„ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}')

    # ìµœì¢… ìš”ì•½
    if results:
        df_r = pd.DataFrame(results)
        
        # ìˆ˜ìµë¥  ê¸°ì¤€ ì •ë ¬
        df_sorted = df_r.sort_values('total_return', ascending=False)
        
        # ìƒìœ„ ì „ëµë“¤
        top_strategies = df_sorted.head(10)
        
        log_realtime('\nâœ… ìµœì í™” ì™„ë£Œ')
        log_realtime('=' * 70)
        log_realtime(f'ğŸ“Š 7ê°œì›” í†µí•© ë°ì´í„° ìµœì í™” ê²°ê³¼ ìš”ì•½:')
        log_realtime(f'ì´ í…ŒìŠ¤íŠ¸ ì „ëµ: {len(results):,}ê°œ')
        log_realtime(f'ì–‘ìˆ˜ ìˆ˜ìµë¥  ì „ëµ: {len(df_r[df_r["total_return"] > 0]):,}ê°œ')
        log_realtime(f'ìŒìˆ˜ ìˆ˜ìµë¥  ì „ëµ: {len(df_r[df_r["total_return"] <= 0]):,}ê°œ')
        
        log_realtime('\nğŸ† TOP 10 ì „ëµ (7ê°œì›” í†µí•© ìˆ˜ìµë¥ ):')
        log_realtime('=' * 100)
        log_realtime(f"{'ìˆœìœ„':<4} {'ì „ëµí‚¤':<35} {'ìˆ˜ìµë¥ ':<10} {'MDD':<8} {'ê±°ë˜ìˆ˜':<8} {'ìŠ¹ë¥ ':<8} {'ìƒ¤í”„ë¹„ìœ¨':<8}")
        log_realtime('-' * 100)
        
        for i, (_, row) in enumerate(top_strategies.iterrows(), 1):
            log_realtime(f"{i:2d}.  {row['key']:<35} {row['total_return']:+8.2f}% {row['mdd']:6.2f}% {row['trades']:8d} {row['win_rate']:6.1f}% {row['sharpe_ratio']:7.2f}")
        
        log_realtime('-' * 100)
        
        # ìµœê³  ì„±ê³¼ ì „ëµ ìƒì„¸ ë¶„ì„
        best_strategy = df_sorted.iloc[0]
        log_realtime(f'\nğŸ¥‡ ìµœê³  ì„±ê³¼ ì „ëµ: {best_strategy["key"]}')
        log_realtime(f'   7ê°œì›” í†µí•© ìˆ˜ìµë¥ : {best_strategy["total_return"]:+.2f}%')
        log_realtime(f'   ìµœëŒ€ MDD: {best_strategy["mdd"]:.2f}%')
        log_realtime(f'   ì´ ê±°ë˜ ìˆ˜: {best_strategy["trades"]:,}ê°œ')
        log_realtime(f'   ìŠ¹ë¥ : {best_strategy["win_rate"]:.1f}%')
        log_realtime(f'   ìƒ¤í”„ ë¹„ìœ¨: {best_strategy["sharpe_ratio"]:.2f}')
        
        # ìµœì¢… ì €ì¥
        df_sorted.to_csv(RESULTS_CSV, index=False, encoding='utf-8-sig')
        with open(RESULTS_JSON, 'w', encoding='utf-8') as f:
            json.dump({
                'results': results,
                'summary': {
                    'total_strategies': len(results),
                    'positive_strategies': len(df_r[df_r["total_return"] > 0]),
                    'top_10_strategies': top_strategies.to_dict('records'),
                    'best_strategy': best_strategy.to_dict()
                },
                'test_period': '2025-01 to 2025-07',
                'generated_time': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        # í”ŒëŸ¬ìŠ¤ ê²°ê³¼ë§Œ ë”°ë¡œ ìµœì¢… ì €ì¥
        positive_results = [r for r in results if r['total_return'] > 0]
        if positive_results:
            df_positive = pd.DataFrame(positive_results).sort_values('total_return', ascending=False)
            df_positive.to_csv(POSITIVE_RESULTS_CSV, index=False, encoding='utf-8-sig')
            with open(POSITIVE_RESULTS_JSON, 'w', encoding='utf-8') as f:
                json.dump({
                    'positive_results': positive_results,
                    'count': len(positive_results),
                    'top_10_positive': df_positive.head(10).to_dict('records'),
                    'best_positive': df_positive.iloc[0].to_dict() if len(df_positive) > 0 else None,
                    'test_period': '2025-01 to 2025-07',
                    'generated_time': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
        
        log_realtime(f'\nğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:')
        log_realtime(f'   ì „ì²´ CSV: {RESULTS_CSV}')
        log_realtime(f'   ì „ì²´ JSON: {RESULTS_JSON}')
        if positive_results:
            log_realtime(f'   í”ŒëŸ¬ìŠ¤ CSV: {POSITIVE_RESULTS_CSV}')
            log_realtime(f'   í”ŒëŸ¬ìŠ¤ JSON: {POSITIVE_RESULTS_JSON}')
            log_realtime(f'   í”ŒëŸ¬ìŠ¤ ì „ëµ ìˆ˜: {len(positive_results):,}ê°œ')
        
    else:
        log_realtime('âŒ ê²°ê³¼ ì—†ìŒ')


if __name__ == '__main__':
    main()
