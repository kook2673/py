# -*- coding: utf-8 -*-
"""
1ë¶„/3ë¶„/5ë¶„ ìŠ¤ìº˜í•‘(ì¼ë°˜í™” MA) ìµœì í™” ìŠ¤í¬ë¦½íŠ¸ - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë²„ì „
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- ì¦ë¶„ ì €ì¥ìœ¼ë¡œ I/O ë¶€í•˜ ìµœì†Œí™”
- ì§„í–‰ìƒíƒœ ì••ì¶• ì €ì¥ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

ì‚¬ìš©ë²•:
  python optimizer_batch.py
"""

import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
import gc
import os

from config import DATA_DIR, CSV_PATTERN, MAX_WORKERS_LIMIT
from utils import (
    load_progress, save_progress, log_realtime, 
    generate_param_grid, save_profitable_results
)
from worker import get_worker_initializer, get_single_runner


class BatchOptimizer:
    """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, batch_size=1000):
        self.batch_size = batch_size
        self.results = []
        self.total_processed = 0
        
    def save_batch_results(self, batch_results: list, batch_num: int):
        """ë°°ì¹˜ ê²°ê³¼ë¥¼ ì¦ë¶„ ì €ì¥"""
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            batch_dir = os.path.join(script_dir, 'logs', 'batch_results')
            os.makedirs(batch_dir, exist_ok=True)
            
            # ë°°ì¹˜ë³„ CSV ì €ì¥
            batch_csv = os.path.join(batch_dir, f'batch_{batch_num:04d}.csv')
            import pandas as pd
            df_batch = pd.DataFrame(batch_results)
            df_batch.to_csv(batch_csv, index=False, encoding='utf-8-sig')
            
            # ë°°ì¹˜ë³„ JSON ì €ì¥
            batch_json = os.path.join(batch_dir, f'batch_{batch_num:04d}.json')
            import json
            with open(batch_json, 'w', encoding='utf-8') as f:
                json.dump({'batch_num': batch_num, 'results': batch_results}, f, ensure_ascii=False, indent=2)
            
            log_realtime(f'ğŸ’¾ ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ: {len(batch_results):,}ê°œ ê²°ê³¼')
            
        except Exception as e:
            log_realtime(f'âš ï¸ ë°°ì¹˜ {batch_num} ì €ì¥ ì‹¤íŒ¨: {e}')
    
    def merge_all_batches(self):
        """ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ë¥¼ ë³‘í•©í•˜ì—¬ ìµœì¢… ì €ì¥"""
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            batch_dir = os.path.join(script_dir, 'logs', 'batch_results')
            
            if not os.path.exists(batch_dir):
                log_realtime('âš ï¸ ë°°ì¹˜ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.')
                return
            
            # ëª¨ë“  ë°°ì¹˜ íŒŒì¼ ì°¾ê¸°
            batch_files = [f for f in os.listdir(batch_dir) if f.endswith('.csv')]
            batch_files.sort()
            
            all_results = []
            for batch_file in batch_files:
                batch_path = os.path.join(batch_dir, batch_file)
                import pandas as pd
                df_batch = pd.read_csv(batch_path, encoding='utf-8-sig')
                all_results.extend(df_batch.to_dict('records'))
            
            log_realtime(f'ğŸ”„ ë°°ì¹˜ ê²°ê³¼ ë³‘í•© ì™„ë£Œ: ì´ {len(all_results):,}ê°œ')
            
            # ìµœì¢… ê²°ê³¼ ì €ì¥
            from utils import save_final_results, save_profitable_results
            
            best_by_return, best_by_mdd = save_final_results(all_results)
            
            # ìˆ˜ìµë¥  ì–‘ìˆ˜ ê²°ê³¼ ì €ì¥
            profitable_results, summary = save_profitable_results(all_results)
            
            return all_results, profitable_results
            
        except Exception as e:
            log_realtime(f'âš ï¸ ë°°ì¹˜ ë³‘í•© ì‹¤íŒ¨: {e}')
            return None, None
    
    def process_batch(self, batch_params, batch_num, max_workers):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
        batch_results = []
        
        with ProcessPoolExecutor(
            max_workers=max_workers, 
            initializer=get_worker_initializer(), 
            initargs=(DATA_DIR, CSV_PATTERN)
        ) as ex:
            futures = {ex.submit(get_single_runner(), p): p for p in batch_params}
            
            for i, fut in enumerate(as_completed(futures), start=1):
                params = futures[fut]
                try:
                    out = fut.result()
                    batch_results.append(out)
                    
                    # ì§„í–‰ìƒí™© ì¶œë ¥
                    sign = 'ğŸŸ¢' if out['total_return'] > 0 else 'ğŸ”´'
                    log_realtime(
                        f"{sign} ë°°ì¹˜{batch_num} | {i}/{len(batch_params)} | "
                        f"tf={out['timeframe_minutes']}m, "
                        f"f={out['fast_ma']}, s={out['slow_ma']}, "
                        f"sl={out['stop_loss_pct']*100:.2f}%, "
                        f"vm={out['volume_multiplier']:.2f}, "
                        f"mfc={out['min_pass_filters']:02d} => "
                        f"ret={out['total_return']:+.2f}% MDD={out['mdd']:.2f}%"
                    )
                    
                except Exception as e:
                    f, s, sl, vm, mfc, tfm = params
                    key = f'f{f}_s{s}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'
                    log_realtime(f'âŒ ì˜¤ë¥˜: {key} - {e}')
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (100ê°œë§ˆë‹¤)
                if i % 100 == 0:
                    gc.collect()
        
        return batch_results


def main():
    """ë©”ì¸ ìµœì í™” ì‹¤í–‰ í•¨ìˆ˜"""
    log_realtime('ğŸš€ ìŠ¤ìº˜í•‘ ìµœì í™” ì‹œì‘ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë²„ì „)')
    log_realtime('=' * 60)
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    BATCH_SIZE = 1000  # 1000ê°œì”© ì²˜ë¦¬
    
    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    grid = generate_param_grid()
    total = len(grid)
    log_realtime(f'ì´ ì¡°í•© ìˆ˜: {total:,}ê°œ')
    log_realtime(f'ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE:,}ê°œ')
    log_realtime(f'ì´ ë°°ì¹˜ ìˆ˜: {total // BATCH_SIZE + (1 if total % BATCH_SIZE else 0):,}ê°œ')
    
    # ì§„í–‰ìƒíƒœ ë¶ˆëŸ¬ì˜¤ê¸°
    progress = load_progress()
    done_set = set(progress.get('done_keys', []))
    
    # ë¯¸ì™„ë£Œ ì¡°í•©ë§Œ í•„í„°ë§
    pending = []
    for f, s, sl, vm, mfc, tfm in grid:
        key = f'f{f}_s{s}_sl{sl}_vm{vm}_mfc{mfc}_tfm{tfm}'
        if key not in done_set:
            pending.append((f, s, sl, vm, mfc, tfm))
    
    log_realtime(f'ëŒ€ê¸° ì‘ì—…: {len(pending):,}ê°œ (ì´ë¯¸ ì™„ë£Œ: {len(done_set):,}ê°œ)')
    
    if not pending:
        log_realtime('âœ… ì§„í–‰í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì™„ë£Œ)')
        return
    
    # ì›Œì»¤ ìˆ˜ ê²°ì •
    max_workers = max(1, min((multiprocessing.cpu_count() or 2) - 1, MAX_WORKERS_LIMIT))
    log_realtime(f'ë³‘ë ¬ ì›Œì»¤ ìˆ˜: {max_workers}ê°œ')
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
    optimizer = BatchOptimizer(batch_size=BATCH_SIZE)
    batch_num = 1
    
    for i in range(0, len(pending), BATCH_SIZE):
        batch_params = pending[i:i + BATCH_SIZE]
        log_realtime(f'\nğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹œì‘: {len(batch_params):,}ê°œ')
        log_realtime('=' * 50)
        
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_results = optimizer.process_batch(batch_params, batch_num, max_workers)
        
        # ë°°ì¹˜ ê²°ê³¼ ì €ì¥
        optimizer.save_batch_results(batch_results, batch_num)
        
        # ì§„í–‰ìƒíƒœ ì—…ë°ì´íŠ¸
        for result in batch_results:
            done_set.add(result['key'])
        
        progress['done_keys'] = list(done_set)
        progress['last_time'] = datetime.now().isoformat()
        progress['total'] = total
        progress['batches_completed'] = batch_num
        save_progress(progress)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del batch_results
        gc.collect()
        
        log_realtime(f'âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ | ì´ ì§„í–‰ë¥ : {len(done_set)/total*100:.1f}%')
        batch_num += 1
    
    # ëª¨ë“  ë°°ì¹˜ ë³‘í•©
    log_realtime('\nğŸ”„ ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ ë³‘í•© ì¤‘...')
    all_results, profitable_results = optimizer.merge_all_batches()
    
    if all_results:
        log_realtime('\nâœ… ìµœì í™” ì™„ë£Œ')
        log_realtime('=' * 60)
        log_realtime(f'ì´ ì²˜ë¦¬ëœ ì¡°í•©: {len(all_results):,}ê°œ')
        log_realtime(f'ìˆ˜ìµë¥  ì–‘ìˆ˜ ì „ëµ: {len(profitable_results) if profitable_results else 0:,}ê°œ')
    else:
        log_realtime('âŒ ê²°ê³¼ ë³‘í•© ì‹¤íŒ¨')


if __name__ == '__main__':
    main()
